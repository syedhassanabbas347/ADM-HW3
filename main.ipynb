{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing all the libraries that are needed for solving this homework \n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import collections\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ist we decided to download data files on PC's of us separately\n",
    "#than we come up with more efficient way of downloading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that we have 3 documents to parse. This function get the URL of the 3 documents provided, and store them in liste2\n",
    "#we store the url in a dataframe to let us drop the url that does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part we work on the crawling of wikipedia we dowloaded the data from the website and stored it on local machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_parse(number_of_file_to_parse):    \n",
    "    liste2 = []\n",
    "    for k in range(1, number_of_file_to_parse+1):\n",
    "        i = str(k)\n",
    "        url = 'https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies'+i+'.html'\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for par in soup.select('a'):\n",
    "            liste2.append(par.text)\n",
    "    df= pd.DataFrame(liste2)\n",
    "    df = df.drop(df.index[[9429, 9671,15520,15576,17725,18100,21267,23664,25240,25873,27675,27721,27768,28053,28180,28273,28378,29229]])\n",
    "    df = df.reset_index()\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need this function to separate some proper names that will appear in some values of the dictionary,\n",
    "# for example the value associated to the key \"Starring\"\n",
    "def SepName(s):\n",
    "\n",
    "    for i in range(1, len(s)):\n",
    "        if s[i].isupper() and s[i - 1] != \" \":\n",
    "            s = s[:i] + \" \" + s[i:]\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part trys to extract the intro and plot of the film, if they aren't into the page they are set to NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function parses the intro\n",
    "def Intro( html, dic):\n",
    "\n",
    "    # We start from the table because the intro is always on the same level of the table\n",
    "    start = html.find( \"table\", {\"class\" : \"infobox vevent\"} )\n",
    "\n",
    "    tag = \"p\"\n",
    "    intro = \"\"\n",
    "\n",
    "    # Then we add all the intro paragraphs text in the string \"intro\";\n",
    "    # when the first tag different from \"p\" is found, the loop ends.\n",
    "    # There is always an intro so we don't need to set the intro to NA in advance\n",
    "    while tag == \"p\":\n",
    "\n",
    "        paragraph = start.find_next_sibling()\n",
    "        tag = paragraph.name\n",
    "\n",
    "        if tag == \"p\":\n",
    "            intro += paragraph.text\n",
    "\n",
    "        start = paragraph\n",
    "\n",
    "    dic[\"Intro\"] = intro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function parses the plot\n",
    "def Plot( html, dic):\n",
    "\n",
    "    dic[\"Plot\"] = \"NA\"\n",
    "\n",
    "    # Here we start from \"span\" {\"class\": \"mw-headline\"} because it indicates the title of a new Wikipedia section\n",
    "    start = html.find(\"span\", {\"class\": \"mw-headline\"})\n",
    "\n",
    "    # If there is a section linked to the plot, we take it; otherwise we leave the value set to NA and we do nothing\n",
    "    # in the exception with \"pass\"\n",
    "    try:\n",
    "\n",
    "        # We scroll through all the sections until we find one named in the following way.\n",
    "        # These are all the possible titles of the sections linked to the plot, found by examining all the files\n",
    "        while start[\"id\"] != \"Plot\" and start[\"id\"] != \"Plot_summary\" and start[\"id\"] != \"Premise\" and start[\n",
    "            \"id\"] != \"Summary\":\n",
    "            start = start.find_next(\"span\", {\"class\": \"mw-headline\"})\n",
    "\n",
    "        # We go back up the hierarchy of a level because the paragraphs containing the plot are on the same level\n",
    "        # as the \"h2\" tags, which contain \"span\": this because later we want to scroll the siblings\n",
    "        # in the same way proposed in Plot function\n",
    "        start = start.find_parent()\n",
    "        tag = \"p\"\n",
    "        plot = \"\"\n",
    "\n",
    "        while tag == \"p\":\n",
    "\n",
    "            paragraph = start.find_next_sibling()\n",
    "            tag = paragraph.name\n",
    "\n",
    "            if tag == \"p\":\n",
    "                plot += paragraph.text\n",
    "\n",
    "            start = paragraph\n",
    "\n",
    "        dic[\"Plot\"] = plot\n",
    "\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting data from crawled HTML pages and transform it into a film.tsv file with the extracted informations as specified\n",
    "# into the requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function parses the info box\n",
    "def Info( html, dic):\n",
    "\n",
    "    # Here we start from the info box itself\n",
    "    info_box = html.find(\"table\", {\"class\": \"infobox vevent\"})\n",
    "\n",
    "    # The first content with the \"tr\" tag is always the name of the movie\n",
    "    start = info_box.find(\"tr\")\n",
    "    dic[\"Film name\"] = start.text\n",
    "\n",
    "    # We find the next \"th\" tags because they are the ones containing the requested information:\n",
    "    # usually there is an image of the movie poster that in this way is skipped since it has tag \"td\"\n",
    "    start = start.find_next(\"th\")\n",
    "    start = start.find_parent()\n",
    "    tag = \"tr\"\n",
    "\n",
    "    # We set all values ​​to NA in advance\n",
    "    dic[\"Director\"] = dic[\"Producer\"] = dic[\"Writer\"] = dic[\"Starring\"] = dic[\"Music\"] = dic[\"Release date\"] = dic[\n",
    "        \"Runtime\"] = dic[\"Country\"] = dic[\"Language\"] = dic[\"Budget\"] = \"NA\"\n",
    "\n",
    "    # Here too we scroll all the \"tr\" tags and analyze their content\n",
    "    while (tag == \"tr\"):\n",
    "\n",
    "        line = start.find_next_sibling()\n",
    "\n",
    "        # Instead the information is always contained at a lower level than the variable \"line\".\n",
    "        # If these informations are somehow not contained in the html file,\n",
    "        # we risk an infinite loop: the exception is for this\n",
    "        try:\n",
    "            info = line.find_next()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        tag = line.name\n",
    "        start = line\n",
    "\n",
    "        # We save the information of interest in the dictionary\n",
    "        if info.text == \"Directed by\":\n",
    "            dic[\"Director\"] = SepName(info.find_next(\"td\").text)\n",
    "\n",
    "        elif info.text == \"Produced by\":\n",
    "            dic[\"Producer\"] = SepName(info.find_next(\"td\").text)\n",
    "\n",
    "        elif info.text == \"Written by\":\n",
    "            dic[\"Writer\"] = SepName(info.find_next(\"td\").text)\n",
    "\n",
    "        elif info.text == \"Starring\":\n",
    "            dic[\"Starring\"] = SepName(info.find_next(\"td\").text)\n",
    "\n",
    "        elif info.text == \"Music by\":\n",
    "            dic[\"Music\"] = SepName(info.find_next(\"td\").text)\n",
    "\n",
    "        elif info.text == \"Release date\":\n",
    "            dic[\"Release date\"] = info.find_next(\"td\").text\n",
    "\n",
    "        elif info.text == \"Running time\":\n",
    "            dic[\"Runtime\"] = info.find_next(\"td\").text\n",
    "\n",
    "        elif info.text == \"Country\":\n",
    "            dic[\"Country\"] = info.find_next(\"td\").text\n",
    "\n",
    "        elif info.text == \"Language\":\n",
    "            dic[\"Language\"] = SepName(info.find_next(\"td\").text)\n",
    "\n",
    "        elif info.text == \"Budget\":\n",
    "            dic[\"Budget\"] = info.find_next(\"td\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this part clean the data for search engine we remove all the data that was not helpful in search engine while \n",
    "#searching through documents e.g.stopwords, punctuation and also did Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(k):\n",
    "    \n",
    "    \n",
    "    k = k.lower() # make all the word turn into lower case\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    result = tokenizer.tokenize(k)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = result\n",
    "    new_sentence =[]\n",
    "    for w in text:\n",
    "        if w not in stop_words: \n",
    "            new_sentence.append(w) #drop the stopwords of our string\n",
    "    ps = PorterStemmer()\n",
    "    final = []\n",
    "    for word in new_sentence:\n",
    "        final.append(ps.stem(word)) #get the root form of each word\n",
    "    return(final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Conjunctive query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part makes the indexing of the extracted informations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1) Create your index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we create the index \n",
    "def index_1():\n",
    "\n",
    "    listemot = [] #will contain the words \n",
    "    listedoc = [] # will be a list of lists. Each list in this list is link to a unique word, and will contain the document in which the word appears\n",
    "\n",
    "    for k in range(0,29982):\n",
    "        if k%100 == 0 : \n",
    "            print(k) # indicator to see at which step we are during the computing\n",
    "        try : \n",
    "            nbr = str(k)\n",
    "            with open(r'C:\\Users\\danyl\\OneDrive\\Documents\\tsv\\document_'+nbr+'.tsv', encoding = 'utf8') as tsvfile:\n",
    "                reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "                for row in reader:\n",
    "                    if len(row)>0:\n",
    "                        a = row[1] # we get the intro\n",
    "                        b = row[2] #we get the plot\n",
    "            b = preprocess(b) # we drop the stopwords, ponctuation, and turn words into root form\n",
    "            a = preprocess(a)\n",
    "            for element in b: #we iterering over the words in the plot\n",
    "                if element in listemot: #we check if the word is already in the list\n",
    "                    i = listemot.index(element) #if it is, we get its index \n",
    "                    if nbr not in listedoc[i]: #we check if we don't already refer that the word is in this document\n",
    "                        listedoc[i].append(nbr) #we save the tsv number in a list which is link to this specific word.\n",
    "                else : \n",
    "                    listemot.append(element) #we add the word in the list if it is not already in\n",
    "                    ldoc = [nbr] #we store the number of the tsv file\n",
    "                    listedoc.append(ldoc) # we store the new list in the list of document\n",
    "            for element in a: #we do the same thing we the intro, with the same logic\n",
    "                if element in listemot:\n",
    "                    i = listemot.index(element)\n",
    "                    if nbr not in listedoc[i]:\n",
    "                        listedoc[i].append(nbr)\n",
    "                else : \n",
    "                    listemot.append(element)\n",
    "                    ldoc = [nbr]\n",
    "                    listedoc.append(ldoc)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    a = {}\n",
    "    # we create a dictionnary. the keys of the dictionnary is the index of each word, and the value is a list of document in which the word appears\n",
    "    for k in range(0,len(listemot)):\n",
    "        a[k] = listedoc[k]\n",
    "    \n",
    "    return(a,listemot, listedoc)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is similar to the code we have done to build the first index. We just have two difference\n",
    "def index_2():\n",
    "    listemot2 = []\n",
    "    listedoc2 = []\n",
    "    dic_words_doc = {} # will contain, for each document, the number of words it contain\n",
    "\n",
    "    for k in range(0,29982):\n",
    "        if k%100 == 0:\n",
    "            print(k)\n",
    "        try : \n",
    "            nbr = str(k)\n",
    "            with open(r'C:\\Users\\danyl\\OneDrive\\Documents\\tsv\\document_'+nbr+'.tsv', encoding = 'utf8') as tsvfile:\n",
    "                reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "                for row in reader:\n",
    "                    if len(row)>0:\n",
    "                        a = row[1]\n",
    "                        b = row[2]\n",
    "            b = preprocess(b)\n",
    "            a = preprocess(a)\n",
    "            for element in b: \n",
    "                if element in listemot2:\n",
    "                    i = listemot2.index(element)\n",
    "                    listedoc2[i].append(nbr) #the difference is that even if we have already stored the document that contain the word, we are going to store it again if we have the same word several time in the same document. So we have informations on the occurence of each word in each document\n",
    "                else : \n",
    "                    listemot2.append(element)\n",
    "                    ldoc = [nbr]\n",
    "                    listedoc2.append(ldoc)\n",
    "            for element in a: \n",
    "                if element in listemot2:\n",
    "                    i = listemot2.index(element)\n",
    "                    listedoc2[i].append(nbr)\n",
    "                else : \n",
    "                    listemot2.append(element)\n",
    "                    ldoc = [nbr]\n",
    "                    listedoc2.append(ldoc)\n",
    "            dic_words_doc[k] = len(a) + len(b) # here we store the number of words in the intro+plot\n",
    "        \n",
    "        except : \n",
    "            pass\n",
    "    listeinverted = []\n",
    "\n",
    "    # here we want to create a dictionnary in which the key will be the index of the word, and the value will be a dictionnary (key = document, value = tf-idf)\n",
    "\n",
    "    for k in range(len(listedoc2)): #listedoc2 contain lists of documents link a unique word\n",
    "        i = collections.Counter(listedoc2[k]) # We now have a dictionnary (key = document, value = occurence of the word in this document)\n",
    "        dic = {} \n",
    "        for key, value in i.items(): # we go into the dictionnary i we created\n",
    "            number = dic_words_doc[int(key)] # we get the number of words of the document which is the key of the dictionnary\n",
    "            dic[key] = (value/number)*(1+math.log(float(29981/(len(i))))) # we calculate the tf-idf of each document\n",
    "        listeinverted.append(dic) #we store the dic that refers to a word and that containt (key = document, value = tf_idf) in a list\n",
    "    \n",
    "\n",
    "    # here we create the dictionnary (key = index_word, value = dictionnary(key = document, value = tf-idf))\n",
    "    dicInverted = {}\n",
    "\n",
    "    for k in range(0,len(listemot2)):\n",
    "        dicInverted[k] = listeinverted[k]\n",
    "    return(dicInverted, listemot2, listedoc2, dic_words_doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Algorithmic question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined a function to find maximum of two values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum(x,y):\n",
    "    if x>y:\n",
    "        return x\n",
    "    else:\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_palindromic_seq(str): \n",
    "    n = len(str) \n",
    "  \n",
    "    # Creating a table to store results of subproblems \n",
    "    L = [[0 for x in range(n)] for x in range(n)] \n",
    "  \n",
    "    # Strings of length 1 are palindrome of length 1 \n",
    "    for i in range(n): \n",
    "        L[i][i] = 1\n",
    "   \n",
    "    for c in range(2, n+1): \n",
    "        for i in range(n-c+1): \n",
    "            j = i+c-1\n",
    "            if str[i] == str[j] and c == 2: \n",
    "                L[i][j] = 2\n",
    "            elif str[i] == str[j]: \n",
    "                L[i][j] = L[i+1][j-1] + 2\n",
    "            else: \n",
    "                L[i][j] = maximum(L[i][j-1], L[i+1][j]); \n",
    "  \n",
    "    return L[0][n-1] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# testing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the Longest palindromic sequence is 7\n"
     ]
    }
   ],
   "source": [
    "seq = \"DATAMININGSAPIENZA\"\n",
    "print(\"The length of the Longest palindromic sequence is \" + str(long_palindromic_seq(seq)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
